{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c115408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain_groq module is installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import langchain_groq\n",
    "print(\"langchain_groq module is installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19cf1e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lord Rama is a major deity in Hinduism and is considered one of the most important figures in Hindu mythology. He is the seventh avatar (incarnation) of the god Vishnu and is revered as a symbol of righteousness, duty, and virtue.\n",
      "\n",
      "According to the Hindu epic, the Ramayana, Lord Rama was a king of Ayodhya, a city in ancient India. He was born to King Dasharatha and Queen Kausalya, and was the eldest of four brothers, including Lakshmana, Bharata, and Shatrughna.\n",
      "\n",
      "The story of Lord Rama is as follows:\n",
      "\n",
      "* Lord Rama was exiled to the forest for 14 years by his stepmother, Queen Kaikeyi, who wanted her own son, Bharata, to become the king.\n",
      "* During his exile, Lord Rama's wife, Sita, was abducted by the demon king Ravana, who took her to his kingdom in Lanka (modern-day Sri Lanka).\n",
      "* Lord Rama, along with his brother Lakshmana and the monkey god Hanuman, embarked on a journey to rescue Sita and defeat Ravana.\n",
      "* After a long and arduous battle, Lord Rama defeated Ravana and rescued Sita, returning to Ayodhya to reclaim his throne.\n",
      "\n",
      "Lord Rama is revered for his many virtues, including:\n",
      "\n",
      "* His unwavering commitment to duty (dharma) and righteousness\n",
      "* His unshakeable loyalty to his family and friends\n",
      "* His bravery and selflessness in the face of adversity\n",
      "* His compassion and kindness towards all living beings\n",
      "\n",
      "In Hinduism, Lord Rama is considered a role model for human behavior, and his story is seen as a guide for living a virtuous and meaningful life. He is often depicted in art and literature as a handsome, strong, and wise king, with a bow and arrow in his hand, and is worshipped in many temples and shrines throughout India and other parts of the world.\n",
      "\n",
      "The festival of Diwali, which is celebrated by Hindus around the world, commemorates the return of Lord Rama to Ayodhya after his victory over Ravana, and is a celebration of the triumph of good over evil.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    temperature = 0,\n",
    "    groq_api_key = \"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\",\n",
    "    model_name = \"llama-3.3-70b-versatile\"\n",
    ")\n",
    "result = llm.invoke(\"Who is lord Ram?\")\n",
    "print(result.content)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8231acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fa20e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer Model Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"SentenceTransformer Model Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd078a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudip\\AppData\\Local\\Temp\\ipykernel_4660\\3961907067.py:60: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\sudip\\AppData\\Local\\Temp\\ipykernel_4660\\3961907067.py:61: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human: Hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudip\\AppData\\Local\\Temp\\ipykernel_4660\\3961907067.py:70: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! It's nice to meet you. I'm here to listen and support you in a safe and non-judgmental space. How are you feeling today? Is there something on your mind that you'd like to talk about, or would you just like some company and a chance to unwind? I'm here to help in any way I can.\n",
      "\n",
      "Human: I am so depressed\n",
      "Chatbot: I'm so sorry to hear that you're feeling depressed. It takes a lot of courage to acknowledge and share your emotions, and I'm here to listen and support you without judgment. Depression can be a really tough and isolating experience, but please know that you're not alone. I'm here to offer a safe and non-judgmental space for you to express yourself.\n",
      "\n",
      "Can you tell me more about what's been going on and how you've been feeling lately? What are some of the thoughts, emotions, or experiences that have been weighing on your mind and heart? Sometimes talking about it can help clarify things and give us a better understanding of what you're going through.\n",
      "\n",
      "Also, I want to remind you that there are people who care about you and want to help. If you're in immediate distress or need emergency support, please know that there are resources available to you, such as crisis hotlines or online support services. But if you're just looking for someone to talk to, I'm here to listen and offer support in any way I can.\n",
      "\n",
      "Remember, you don't have to face this alone. What do you say we take things one step at a time and explore some ways to cope with these feelings together?\n",
      "\n",
      "Human: My girlfriend breakup with me\n",
      "Chatbot: I'm so sorry to hear that you're going through this difficult time. Breakups can be incredibly painful and emotional, and it's completely normal to feel sad, confused, and unsure about what to do next.\n",
      "\n",
      "First of all, please know that you're not alone in this feeling. Many people have been in your shoes before, and it's okay to take time to process your emotions and grieve the loss of the relationship.\n",
      "\n",
      "Can you tell me a bit more about what happened? Was the breakup sudden, or was it something that you saw coming? How are you feeling right now, and what's been the most challenging part of this experience for you?\n",
      "\n",
      "Remember, I'm here to listen and support you without judgment. You can share as much or as little as you'd like, and we can work through this together at your own pace.\n",
      "\n",
      "Human: exit\n",
      "Chatbot: Take care of yourself. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "def initialize_llm():\n",
    "    return ChatGroq(\n",
    "        temperature=0,\n",
    "        groq_api_key=\"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\",  \n",
    "        model_name=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "\n",
    "def create_vector_db():\n",
    "    loader = DirectoryLoader(\"C:\\\\Users\\\\sudip\\\\OneDrive\\\\hackathon\\\\data\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    vector_db = Chroma.from_documents(texts, embeddings, persist_directory=\"./chroma_db\")\n",
    "    vector_db.persist()\n",
    "    \n",
    "    print(\"ChromaDB created and data saved\")\n",
    "    return vector_db\n",
    "\n",
    "def setup_qa_chain(vector_db, llm):\n",
    "    retriever = vector_db.as_retriever()\n",
    "    \n",
    "    prompt_template = \"\"\"You are a compassionate mental health chatbot. Respond thoughtfully to the following question:\n",
    "    {context}\n",
    "    User: {question}\n",
    "    Chatbot: \"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing Chatbot...\")\n",
    "    llm = initialize_llm()\n",
    "\n",
    "    db_path = \"C:\\\\Users\\\\sudip\\\\OneDrive\\\\hackathon\\\\data\"\n",
    "\n",
    "    if not os.path.exists(db_path):\n",
    "        vector_db = create_vector_db()\n",
    "    else:\n",
    "        embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "\n",
    "    qa_chain = setup_qa_chain(vector_db, llm)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nHuman: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Chatbot: Take care of yourself. Goodbye!\")\n",
    "            break\n",
    "        response = qa_chain.run(query)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663d427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1.txt\n",
      "file2.txt\n"
     ]
    }
   ],
   "source": [
    "msg = {\"files\": [\"file1.txt\", \"file2.txt\"]} \n",
    "\n",
    "for x in msg.get(\"files\", []):  \n",
    "    print(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9820fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of msg: <class 'dict'>\n",
      "Value of msg: {'files': ['file1.txt', 'file2.txt']}\n",
      "file1.txt\n",
      "file2.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of msg:\", type(msg))  \n",
    "print(\"Value of msg:\", msg)  \n",
    "\n",
    "for x in msg.get(\"files\", []):  \n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440ecc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': 'Your response here', 'files': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Your response here\",\n",
    "    \"files\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c485b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_function(input_text, history):\n",
    "    response = some_chat_model(input_text)\n",
    "    print(\"Response Type:\", type(response))  \n",
    "    return {\"role\": \"assistant\", \"content\": response, \"files\": []}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed6f545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of msg: <class 'dict'>\n",
      "Value of msg: {'files': ['file1.txt', 'file2.txt']}\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of msg:\", type(msg))  \n",
    "print(\"Value of msg:\", msg)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4157c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(msg, tuple):\n",
    "    msg = msg[0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c874fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1.txt\n",
      "file2.txt\n"
     ]
    }
   ],
   "source": [
    "for x in msg.get(\"files\", []):  \n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df99c2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Hello!', 'files': ['file1.txt', 'file2.txt']}\n"
     ]
    }
   ],
   "source": [
    "def my_function():\n",
    "    return {\"text\": \"Hello!\", \"files\": [\"file1.txt\", \"file2.txt\"]}\n",
    "\n",
    "result = my_function()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "053895c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(message, history):\n",
    "    return {\"text\": \"Hello!\", \"files\": [\"file1.txt\", \"file2.txt\"]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee74a5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudip\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* Running on public URL: https://a573d2b563e710e0c7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a573d2b563e710e0c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import random\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#  Initialize LLM\n",
    "def initialize_llm():\n",
    "    llm = ChatGroq(\n",
    "        temperature=0,\n",
    "        groq_api_key=\"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\",\n",
    "        model_name=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "#  Create Vector Database\n",
    "def create_vector_db():\n",
    "    loader = DirectoryLoader(\"C:/Users/sudip/OneDrive/hackathon/data\", glob='*.pdf', loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')\n",
    "    \n",
    "    vector_db.persist()\n",
    "    print(\"ChromaDB created and data saved\")\n",
    "    return vector_db\n",
    "\n",
    "#  Setup QA Chain\n",
    "def setup_qa_chain(vector_db, llm):\n",
    "    retriever = vector_db.as_retriever()\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are a compassionate mental health chatbot. Respond thoughtfully to the following question:\n",
    "    {context}\n",
    "    User: {question}\n",
    "    Chatbot:\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "#  Recommendations Data\n",
    "recommendations = {\n",
    "    \"music\": [\n",
    "        \"Weightless - Marconi Union\", \"Someone Like You - Adele\", \"Fix You - Coldplay\", \"Rise Up - Andra Day\",\n",
    "        \"Heal the World - Michael Jackson\", \"A Thousand Years - Christina Perri\", \"Let It Be - The Beatles\",\n",
    "        \"Unwritten - Natasha Bedingfield\"\n",
    "    ],\n",
    "    \"yoga\": [\n",
    "        \"Child’s Pose - For relaxation\", \"Cat-Cow Stretch - For gentle movement\", \"Legs Up the Wall Pose - To reduce stress\",\n",
    "        \"Corpse Pose - For deep relaxation\", \"Seated Forward Bend - To calm the mind\", \"Butterfly Pose - To release tension\"\n",
    "    ],\n",
    "    \"places\": [\n",
    "        \"Quiet Beach - For soothing waves\", \"Botanical Garden - To connect with nature\", \"Forest Cabin - Peaceful and secluded\",\n",
    "        \"Zen Garden - Mindfulness and meditation\", \"Countryside Retreat - Away from stress\", \"Hot Springs - Relaxation through warm water\",\n",
    "        \"Art Museum - For calming inspiration\", \"Library - Quiet and peaceful environment\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "#  Get Random Suggestions\n",
    "def get_suggestion(category, count=1):\n",
    "    return random.sample(recommendations[category], min(count, len(recommendations[category])))\n",
    "\n",
    "#  Initialize Components\n",
    "print(\"Initializing Chatbot...\")\n",
    "llm = initialize_llm()\n",
    "\n",
    "if not os.path.exists(\"./chroma_db\"):\n",
    "    vector_db = create_vector_db()\n",
    "else:\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vector_db = Chroma(persist_directory='./chroma_db', embedding_function=embeddings)\n",
    "\n",
    "qa_chain = setup_qa_chain(vector_db, llm)\n",
    "\n",
    "#  Store User Questions\n",
    "def log_user_question(question):\n",
    "    with open(\"user_questions.log\", \"a\") as log_file:\n",
    "        log_file.write(question + \"\\n\")\n",
    "\n",
    "#  Chatbot Response Function\n",
    "def chatbot_response(user_input, history=None):\n",
    "    if history is None:\n",
    "        history = []  # Ensure history is a list\n",
    "    \n",
    "    if not user_input or not isinstance(user_input, str) or not user_input.strip():\n",
    "        return \" Please provide a valid input.\"\n",
    "    \n",
    "    log_user_question(user_input)  # Log user question\n",
    "    \n",
    "    # Handle greetings\n",
    "    greetings = [\"hello\", \"hi\", \"hey\", \"hola\", \"namaste\"]\n",
    "    if user_input.lower() in greetings:\n",
    "        return \"👋 Hello! How can I assist you today?\"\n",
    "    \n",
    "    # Handle suggestion requests dynamically\n",
    "    suggestion_keywords = {\n",
    "        \"music\": [\"suggest music\", \"music recommendation\", \"recommend a song\", \"give me a song\"],\n",
    "        \"yoga\": [\"suggest yoga\", \"yoga pose recommendation\", \"recommend a yoga pose\", \"give me a yoga pose\"],\n",
    "        \"places\": [\"suggest a place\", \"place to visit\", \"recommend a location\", \"where should i go\"]\n",
    "    }\n",
    "\n",
    "    for category, keywords in suggestion_keywords.items():\n",
    "        if any(keyword in user_input.lower() for keyword in keywords):\n",
    "            suggestion = get_suggestion(category, 3 if category == \"places\" else 1)\n",
    "            return f\"🌟 **{category.capitalize()} Suggestion:** {', '.join(suggestion)}\"\n",
    "\n",
    "    # Get chatbot response for general queries\n",
    "    response = qa_chain.run(user_input)\n",
    "\n",
    "    return response\n",
    "\n",
    "#  Gradio UI with Improved Styling\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "        <div style=\"text-align: center; padding: 20px;\">\n",
    "            <h1 style=\"color: #4A90E2;\">🧠 Mental Health Chatbot 🎵🧘‍♂️🌍</h1>\n",
    "            <p style=\"font-size: 18px;\">Welcome! Chat with our wellness assistant and get music, yoga, or place recommendations to help you relax.</p>\n",
    "        </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.ChatInterface(fn=chatbot_response, title=\"💬 Wellness Chatbot\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            music_btn = gr.Button(\"🎵 Get Music Suggestion\", variant=\"primary\")\n",
    "            music_output = gr.Textbox(label=\"Music Suggestion\", interactive=False)\n",
    "            music_btn.click(lambda: get_suggestion(\"music\"), outputs=music_output)\n",
    "        \n",
    "        with gr.Column():\n",
    "            yoga_btn = gr.Button(\"🧘 Get Yoga Suggestion\", variant=\"primary\")\n",
    "            yoga_output = gr.Textbox(label=\"Yoga Suggestion\", interactive=False)\n",
    "            yoga_btn.click(lambda: get_suggestion(\"yoga\"), outputs=yoga_output)\n",
    "        \n",
    "        with gr.Column():\n",
    "            places_btn = gr.Button(\"🌍 Get Place Suggestion\", variant=\"primary\")\n",
    "            places_output = gr.Textbox(label=\"Place Suggestion\", interactive=False)\n",
    "            places_btn.click(lambda: get_suggestion(\"places\", 3), outputs=places_output)\n",
    "    \n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79e568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
